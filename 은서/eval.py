import os
import json
import math
import random
import argparse
import numpy as np
from glob import glob
from typing import List, Tuple, Dict, Any
from PIL import Image

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pandas as pd
import torchvision.transforms as T 

# --- [1. code444.pyì—ì„œ í•„ìš”í•œ ìš”ì†Œ import] ---
# ğŸ’¡ [í•µì‹¬] code444.pyì— ì •ì˜ëœ ìµœì¢… í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
from code444 import (
    seed_everything,
    find_jsons,
    UniDSetVLM,      # ğŸ’¡ code444.pyì˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    collate_fn_vlm,  # ğŸ’¡ code444.pyì˜ collate í•¨ìˆ˜
    BestVLM,         # ğŸ’¡ code444.pyì˜ ëª¨ë¸ í´ë˜ìŠ¤
    CFG,
    box_cxcywh_to_xyxy,
    iou_xywh_pixel   # ğŸ’¡ mIoU ê³„ì‚° í•¨ìˆ˜
)

# ğŸ’¡ [ì¶”ê°€] CLIPProcessorëŠ” ëª¨ë¸ ë¡œë“œ ì‹œ í•„ìš”
from transformers import CLIPProcessor 

# --- [2. ëª¨ë¸ ë¡œë“œ Helper (BestVLM ë§ì¶¤)] ---
# ğŸ’¡ [ìˆ˜ì •] Vocabì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  CLIP ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ì„ ë³µì›í•©ë‹ˆë‹¤.
def _load_model_from_ckpt(ckpt_path: str, device: torch.device):
    """ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸(.pth)ì—ì„œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. """
    ckpt = torch.load(ckpt_path, map_location=device)
    
    # 1. í•„ìš”í•œ ì„¤ì •ê°’ ì¶”ì¶œ
    clip_model_name = ckpt.get("clip_model_name", CFG.CLIP_MODEL_NAME)
    dim = ckpt.get("dim", CFG.DIM)
    img_size = ckpt.get("img_size", CFG.IMG_SIZE)
    no_pretrain = ckpt.get("no_pretrain", False)

    # 2. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™” (BestVLMì€ vocab_sizeê°€ í•„ìš” ì—†ìŒ)
    model = BestVLM(clip_model_name=clip_model_name,
                         dim=dim,
                         pretrained_backbone=not no_pretrain,
                         img_size=img_size).to(device)
    
    # 3. CLIP Processor ë¡œë“œ (ë°ì´í„° ë¡œë”© ì‹œ í•„ìš”)
    clip_processor = CLIPProcessor.from_pretrained(clip_model_name)

    model.load_state_dict(ckpt["model_state"])
    model.eval() 
    
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {ckpt_path}")
    # ğŸ’¡ [ìˆ˜ì •] Vocab ëŒ€ì‹  clip_processor ë°˜í™˜
    return model, clip_processor, img_size

# --- [3. mIoU ê³„ì‚°ì„ ìœ„í•œ í‰ê°€ ë£¨í”„] ---
def evaluate_loop(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # 1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ê³¼ CLIP Processor ë¡œë“œ
    model, clip_processor, img_size = _load_model_from_ckpt(args.ckpt, device)

    # 2. 'ê²€ì¦ìš©(valid)' ë°ì´í„° ë¡œë” êµ¬ì„±
    json_files = find_jsons(args.json_dir)
    # ğŸ’¡ [í•µì‹¬] UniDSetVLMì— CLIP Processorë¥¼ ì „ë‹¬
    valid_ds = UniDSetVLM(json_files, args.jpg_dir, clip_processor=clip_processor,
                       img_size=img_size)
    
    valid_dl = DataLoader(valid_ds, batch_size=args.batch_size, shuffle=False,
                          num_workers=args.num_workers, collate_fn=collate_fn_vlm)

    all_ious = [] 
    
    with torch.no_grad(): 
        loop = tqdm(valid_dl, desc="Evaluating", leave=True)
        
        for pixel_values, input_ids, attention_mask, targets, meta in loop:
            # ğŸ’¡ [ì¶”ê°€] collate_fnì´ ë¹ˆ ë°°ì¹˜ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
            if pixel_values is None: continue 
            
            pixel_values = pixel_values.to(device)
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            
            # 3. ëª¨ë¸ ì¶”ë¡ 
            pred = model(pixel_values, input_ids, attention_mask) 

            # 4. ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ IoU ê³„ì‚°
            for i in range(targets.size(0)):
                if targets[i] is not None:
                    W, H = meta[i]["meta_orig_size"] 
                    
                    # (1) ì˜ˆì¸¡ BBox (Normalized -> Pixel)
                    cx, cy, nw, nh = [float(v) for v in pred[i].cpu().numpy().tolist()]
                    pred_x = (cx - nw / 2.0) * W; pred_y = (cy - nh / 2.0) * H
                    pred_w = nw * W; pred_h = nh * H
                    
                    # (2) ì •ë‹µ BBox (Normalized -> Pixel)
                    gt = [float(v) for v in targets[i].numpy().tolist()]
                    gt_x = (gt[0] - gt[2] / 2.0) * W; gt_y = (gt[1] - gt[3] / 2.0) * H
                    gt_w = gt[2] * W; gt_h = gt[3] * H
                    
                    # (3) mIoU ê³„ì‚° (code444.pyì—ì„œ import)
                    iou = iou_xywh_pixel([pred_x, pred_y, pred_w, pred_h], [gt_x, gt_y, gt_w, gt_h])
                    all_ious.append(iou)

    # 5. ìµœì¢… mIoU (í‰ê· ) ê³„ì‚° ë° ì¶œë ¥
    if all_ious:
        mIoU = float(np.mean(all_ious))
        print("=======================================")
        print(f"âœ… [í‰ê°€ ì™„ë£Œ] mIoU: {mIoU:.6f}")
        print("=======================================")
    else:
        print(f"ê²½ê³ : í‰ê°€í•  BBox ì •ë‹µì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. {args.json_dir} ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# --- [4. CLI] ---
def get_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--json_dir", type=str, required=True, help="í‰ê°€ìš© JSON íŒŒì¼ ë””ë ‰í† ë¦¬")
    ap.add_argument("--jpg_dir", type=str, required=True, help="í‰ê°€ìš© JPG ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬")
    ap.add_argument("--ckpt", type=str, required=True, help="í•™ìŠµëœ .pth ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    
    ap.add_argument("--batch_size", type=int, default=CFG.BATCH_SIZE * 2) 
    ap.add_argument("--num_workers", type=int, default=CFG.NUM_WORKERS)
    return ap.parse_args()

def main():
    seed_everything(CFG.SEED)
    args = get_args()
    evaluate_loop(args)

if __name__ == "__main__":
    main()
